{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf8fa48f",
   "metadata": {},
   "source": [
    "# Aula 14 - Vieses em Sistemas de Recomendação - Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f059fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy, Dataset, SVD\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.prediction_algorithms.knns import KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef5f4d",
   "metadata": {},
   "source": [
    "## Vídeos de Apoio\n",
    "\n",
    "https://www.youtube.com/watch?v=OSv5J1EVEqA\n",
    "\n",
    "https://www.youtube.com/watch?v=abGCaK86tY4\n",
    "\n",
    "\n",
    "## Exercícios de Vieses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5eb328",
   "metadata": {},
   "source": [
    "### Download do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-31 14:21:45--  https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/steam-200k.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2268468 (2.2M) [text/plain]\n",
      "Saving to: ‘./steam-200k.csv’\n",
      "\n",
      "./steam-200k.csv    100%[===================>]   2.16M  1.60MB/s    in 1.4s    \n",
      "\n",
      "2025-10-31 14:21:47 (1.60 MB/s) - ‘./steam-200k.csv’ saved [2268468/2268468]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/steam-200k.csv  -O ./steam-200k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e70bd",
   "metadata": {},
   "source": [
    "### Funções comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb9122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./steam-200k.csv\")\n",
    "df['hours_played'] = np.log10(df['hours_played'])/np.log10(df['hours_played']).max()\n",
    "df['user_id'] = df['user_id'].astype(str)\n",
    "df.head()\n",
    "\n",
    "train, test = train_test_split(df, test_size=.3, random_state=42)\n",
    "\n",
    "soma_interacoes_por_jogo = train.groupby('game_title').count()[['hours_played']]\n",
    "soma_interacoes_por_jogo.columns = ['interactions']\n",
    "\n",
    "map_soma = soma_interacoes_por_jogo.to_dict()['interactions']\n",
    "\n",
    "soma_interacoes_por_jogo.head(5)\n",
    "\n",
    "soma_interacoes_por_jogo_ordenado = soma_interacoes_por_jogo.sort_values('interactions', ascending=False)\n",
    "\n",
    "map_id = dict(list(enumerate(soma_interacoes_por_jogo_ordenado.index)))\n",
    "map_id = {v:k for k,v in map_id.items()}\n",
    "\n",
    "jogos_maior_20porcento_nome = soma_interacoes_por_jogo_ordenado.reset_index().values[:11, 0]\n",
    "jogos_menor_20porcento_nome = soma_interacoes_por_jogo_ordenado.reset_index().values[-2680:, 0]\n",
    "\n",
    "jogos_entre_os_20porcento = set(df['game_title']) - set(jogos_maior_20porcento_nome) - set(jogos_menor_20porcento_nome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c014be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_buster_group = []\n",
    "niche_group = []\n",
    "diverse_group = []\n",
    "\n",
    "for user_id in train['user_id'].unique():\n",
    "    interacted_by_user = train[train['user_id'] == user_id]\n",
    "    high_pop = interacted_by_user[interacted_by_user['game_title'].isin(jogos_maior_20porcento_nome)]\n",
    "    lowest_pop = interacted_by_user[interacted_by_user['game_title'].isin(jogos_menor_20porcento_nome)]\n",
    "\n",
    "    if len(high_pop)/len(interacted_by_user) > 0.5:\n",
    "        block_buster_group.append(user_id)\n",
    "    elif len(lowest_pop)/len(interacted_by_user) > 0.5:\n",
    "        niche_group.append(user_id)\n",
    "    else:\n",
    "        diverse_group.append(user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e419a8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_recommendation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mget_recommendation\u001b[49m(model)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_recommendation' is not defined"
     ]
    }
   ],
   "source": [
    "preds = get_recommendation(model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7def6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2446.012039943106, 24.63106397647737, 275.0086012684163)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gap_bb_profile = np.mean([calculate_gap_profile(train, i) for i in block_buster_group])\n",
    "gap_n_profile = np.mean([calculate_gap_profile(train, i) for i in niche_group])\n",
    "gap_d_profile = np.mean([calculate_gap_profile(train, i) for i in diverse_group])\n",
    "gap_bb_profile, gap_n_profile, gap_d_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f024b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_is_relevant(user_id, item_id):\n",
    "    aux = df[df[\"user_id\"] == user_id]\n",
    "    if item_id in list(aux['game_title']):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def calculate_MRR(map_recommendations):\n",
    "\n",
    "    MRR = 0\n",
    "    for user_id in map_recommendations:\n",
    "        user_find_corerect_item = False\n",
    "        for index, (item, score) in enumerate(map_recommendations[user_id]):\n",
    "            if user_find_corerect_item is False:\n",
    "                if item_is_relevant(user_id, item):\n",
    "                    MRR += (1/(index+1))\n",
    "                    user_find_corerect_item = True\n",
    "        \n",
    "    return MRR/len(map_recommendations)\n",
    "\n",
    "\n",
    "def calculate_gap_profile(train, user_id):\n",
    "    list_items = train[train['user_id'] == user_id]['game_title'].values\n",
    "    gap = sum([map_soma[i] for i in list_items])\n",
    "    return gap/len(list_items)\n",
    "    \n",
    "\n",
    "def calculate_gap(list_items):\n",
    "    gap = sum([map_soma.get(i, 0) for i in list_items])\n",
    "    return gap/len(list_items)\n",
    "\n",
    "def calculate_gap_groups(predictions):\n",
    "    gap_bb_rec1 = 0\n",
    "    gap_d_rec1 = 0\n",
    "    gap_n_rec1 = 0\n",
    "\n",
    "    n_bb, n_d, n_n = 0, 0, 0\n",
    "\n",
    "    for user, reclist in zip(test['user_id'].unique()[:200], predictions):\n",
    "        if user in block_buster_group:\n",
    "            n_bb += 1\n",
    "            gap_bb_rec1 += calculate_gap([i[0] for i in predictions[reclist]])\n",
    "        elif user in niche_group:\n",
    "            n_n += 1\n",
    "            gap_n_rec1 += calculate_gap([i[0] for i in predictions[reclist]])\n",
    "        else:\n",
    "            n_d += 1\n",
    "            gap_d_rec1 += calculate_gap([i[0] for i in predictions[reclist]])\n",
    "\n",
    "    gap_bb_rec1 = gap_bb_rec1/n_bb\n",
    "    gap_d_rec1 = gap_d_rec1/n_d\n",
    "    gap_n_rec1 = gap_n_rec1/n_n\n",
    "    \n",
    "    delta_gab_bb = (gap_bb_rec1 - gap_bb_profile )/gap_bb_profile\n",
    "    delta_gab_d = (gap_d_rec1 - gap_d_profile )/gap_d_profile\n",
    "    delta_gab_n = (gap_n_rec1 - gap_n_profile )/gap_n_profile\n",
    "    \n",
    "    return delta_gab_bb, delta_gab_d, delta_gab_n\n",
    "\n",
    "def get_recommendation(model, alpha=0.5):\n",
    "    \n",
    "    prediction_user_map = {}\n",
    "    prediction_user_map_corrected = {}\n",
    "\n",
    "    for user in test['user_id'].unique()[:200]:\n",
    "\n",
    "        data = {\"game_title\": list(set(df[\"game_title\"].unique()))}\n",
    "        user_testset_df = pd.DataFrame(data)\n",
    "        user_testset_df[\"hours_played\"] = 0.0\n",
    "        user_testset_df[\"user_id\"] = user\n",
    "\n",
    "        testset = (\n",
    "            Dataset.load_from_df(\n",
    "                user_testset_df[[\"user_id\", \"game_title\", \"hours_played\"]],\n",
    "                reader=reader,\n",
    "            )\n",
    "            .build_full_trainset()\n",
    "            .build_testset()\n",
    "        )\n",
    "        \n",
    "        pred_list = model.test(testset)\n",
    "        \n",
    "        predictions = sorted(\n",
    "            [(pred.iid, pred.est)for pred in pred_list if ((pred.uid == user))],\n",
    "            key=lambda x: x[1],reverse=True\n",
    "        )\n",
    "        \n",
    "        predictions_corrected = sorted(\n",
    "            [(pred.iid, (1-alpha)*pred.est + (alpha/np.log(map_soma.get(pred.iid, 1)+1)) )for pred in pred_list if ((pred.uid == user))],\n",
    "            key=lambda x: x[1],reverse=True\n",
    "        )\n",
    "        \n",
    "        prediction_user_map[user] = predictions[:10]\n",
    "        prediction_user_map_corrected[user] = predictions_corrected[:10]\n",
    "\n",
    "                \n",
    "    return prediction_user_map, prediction_user_map_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5e826",
   "metadata": {},
   "source": [
    "***Exercício 01:*** Explore como outros modelos se comportam em relação ao viés de popularidade. Tente visualizar a distribuição das recomendações e as métricas estudadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d984fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d30d57",
   "metadata": {},
   "source": [
    "***Exercício 02:*** Qual o efeito provocado pela variação do parâmetro $\\alpha$? Demonstre como variam as métricas de acurácia e viés de popularidade com a variação do parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0b487",
   "metadata": {},
   "source": [
    "## Exercícios de Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9254121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-13 10:26:34--  https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/ratings.dat\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8001::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4318513 (4.1M) [text/plain]\n",
      "Saving to: ‘./ratings.dat’\n",
      "\n",
      "./ratings.dat       100%[===================>]   4.12M  15.4MB/s    in 0.3s    \n",
      "\n",
      "2022-11-13 10:26:36 (15.4 MB/s) - ‘./ratings.dat’ saved [4318513/4318513]\n",
      "\n",
      "--2022-11-13 10:26:36--  https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/movies.dat\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 171308 (167K) [text/plain]\n",
      "Saving to: ‘./movies.dat’\n",
      "\n",
      "./movies.dat        100%[===================>] 167.29K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2022-11-13 10:26:37 (2.54 MB/s) - ‘./movies.dat’ saved [171308/171308]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/ratings.dat  -O ./ratings.dat\n",
    "!wget https://raw.githubusercontent.com/Andre-Sacilotti/recsys_lectures/main/datasets/movies.dat  -O ./movies.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68861141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15749/1518670432.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df_genres = pd.read_csv(\"./movies.dat\", sep=\"::\", names=['item', 'title', 'genres'])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./ratings.dat\", sep=\",\")\n",
    "df_genres = pd.read_csv(\"./movies.dat\", sep=\"::\", names=['item', 'title', 'genres'])\n",
    "train, test = train_test_split(df, test_size=.3, random_state=42)\n",
    "genre_map = {i['item']:i['genres'].split(\"|\") for i in df_genres[['item', 'genres']].to_dict('records')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3497ea6",
   "metadata": {},
   "source": [
    "### Funções Comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af41e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recommendation_distribution(prediction_user_map):\n",
    "    user_rec_distribution = {}\n",
    "    n = 0\n",
    "    for (item, score) in prediction_user_map:\n",
    "        for genre in genre_map[item]:\n",
    "            if genre not in user_rec_distribution:\n",
    "                user_rec_distribution[genre] = 0\n",
    "            n += 1\n",
    "            user_rec_distribution[genre] += 1\n",
    "            \n",
    "    user_rec_distribution = {k: v/n for k, v in sorted(user_rec_distribution.items(), key=lambda item: item[1])}\n",
    "    return user_rec_distribution\n",
    "\n",
    "\n",
    "def user_rank_miscalibration(user_profile_dist, rec_profile_dist, alpha=0.001):\n",
    "    p_g_u = user_profile_dist\n",
    "    q_g_u = rec_profile_dist\n",
    "    \n",
    "    Ckl = 0\n",
    "    for genre, p in p_g_u.items():\n",
    "        q = q_g_u.get(genre, 0.0)\n",
    "        til_q = (1 - alpha) * q + alpha * p\n",
    "\n",
    "        if til_q == 0 or p_g_u.get(genre, 0) == 0:\n",
    "            Ckl = Ckl\n",
    "        else:\n",
    "            Ckl += p * np.log2(p / til_q)\n",
    "    return Ckl\n",
    "\n",
    "def get_mean_rank_miscalibration(predictions):\n",
    "    \n",
    "    MRMC = 0\n",
    "    \n",
    "    for user in predictions:\n",
    "        RMC = 0\n",
    "        user_profile_dist = get_user_profile_distribution(train, user)\n",
    "        if user_profile_dist == {}:\n",
    "            continue\n",
    "        \n",
    "        void = user_rank_miscalibration(user_profile_dist, {})\n",
    "        N = len(predictions[user])\n",
    "        for i in range(1, N):\n",
    "            user_rec_dist = get_user_recommendation_distribution(predictions[user][:i])\n",
    "            kl = user_rank_miscalibration(user_profile_dist, user_rec_dist)\n",
    "            RMC += kl/void\n",
    "\n",
    "        MRMC += RMC/N\n",
    "    \n",
    "    return MRMC/len(predictions)\n",
    "\n",
    "def get_recommendation_raw(model):\n",
    "    \n",
    "    prediction_user_map = {}\n",
    "\n",
    "    for user in test['user'].unique()[:200]:\n",
    "\n",
    "        data = {\"item\": list(set(df[\"item\"].unique()))}\n",
    "        user_testset_df = pd.DataFrame(data)\n",
    "        user_testset_df[\"rating\"] = 0.0\n",
    "        user_testset_df[\"user\"] = user\n",
    "\n",
    "        testset = (\n",
    "            Dataset.load_from_df(\n",
    "                user_testset_df[[\"user\", \"item\", \"rating\"]],\n",
    "                reader=reader,\n",
    "            )\n",
    "            .build_full_trainset()\n",
    "            .build_testset()\n",
    "        )\n",
    "        \n",
    "        pred_list = model.test(testset)\n",
    "        \n",
    "        predictions = sorted(\n",
    "            [(pred.iid, pred.est)for pred in pred_list if ((pred.uid == user))],\n",
    "            key=lambda x: x[1],reverse=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        prediction_user_map[user] = predictions[:10]\n",
    "\n",
    "                \n",
    "    return prediction_user_map\n",
    "\n",
    "def rerank_recommendation(profile_dist, list_recomended_items, user, N, tradeoff):\n",
    "    re_ranked_list = []\n",
    "    re_ranked_with_score = []\n",
    "    \n",
    "    for _ in range(N):\n",
    "        \n",
    "        max_mmr = -np.inf\n",
    "        max_item = None\n",
    "        max_item_rating = None\n",
    "        \n",
    "        for item, rating in list_recomended_items:\n",
    "            if item in re_ranked_list:\n",
    "                continue\n",
    "                \n",
    "            temporary_list = re_ranked_list + [item]\n",
    "            temporary_list_with_score = re_ranked_with_score + [(item, rating)]\n",
    "                \n",
    "            weight_part = sum(\n",
    "                recomendation[1]\n",
    "                for recomendation in temporary_list_with_score\n",
    "            )\n",
    "            \n",
    "            full_tmp_calib = calculate_calibration_sum(\n",
    "                profile_dist,\n",
    "                temporary_list_with_score,\n",
    "                user\n",
    "            )\n",
    "            \n",
    "            maximized = (1 - tradeoff)*weight_part - tradeoff*full_tmp_calib\n",
    "            \n",
    "            if maximized > max_mmr:\n",
    "                max_mmr = maximized\n",
    "                max_item = item\n",
    "                max_item_rating = rating\n",
    "            \n",
    "        if max_item is not None:\n",
    "            re_ranked_list.append(max_item)\n",
    "            re_ranked_with_score.append((max_item, max_item_rating))\n",
    "            \n",
    "    return re_ranked_list, re_ranked_with_score  \n",
    "\n",
    "def calculate_calibration_sum(profile_dist, temporary_list_with_score, user, alpha=0.001):\n",
    "    kl_div = 0.0\n",
    "    reco_distr = get_user_recommendation_distribution(temporary_list_with_score)\n",
    "    for genre, p in profile_dist.items():\n",
    "        q = reco_distr.get(genre, 0.0)\n",
    "        til_q = (1 - alpha) * q + alpha * p\n",
    "\n",
    "        if p == 0.0 or til_q == 0.0:\n",
    "            kl_div = kl_div\n",
    "        else:\n",
    "            kl_div = kl_div + (p * np.log2(p / til_q))\n",
    "    return kl_div\n",
    "\n",
    "def get_user_profile_distribution(df, user):\n",
    "    user_profile_distribution = {}\n",
    "    n = 0\n",
    "    for item in df[df['user'] == user]['item'].values:\n",
    "\n",
    "        for genre in genre_map[item]:\n",
    "            if genre not in user_profile_distribution:\n",
    "                user_profile_distribution[genre] = 0\n",
    "            n += 1\n",
    "            user_profile_distribution[genre] += 1\n",
    "            \n",
    "    user_profile_distribution = {k: v/n for k, v in sorted(user_profile_distribution.items(), key=lambda item: item[1])}\n",
    "    return user_profile_distribution\n",
    "\n",
    "\n",
    "def get_recommendation_fairness(model, lambda_=0.5):\n",
    "    \n",
    "    prediction_user_map = {}\n",
    "\n",
    "    for user in test['user'].unique()[:200]:\n",
    "        \n",
    "        user_profile_distribution = get_user_profile_distribution(train, user)\n",
    "\n",
    "        data = {\"item\": list(set(df[\"item\"].unique()))}\n",
    "        user_testset_df = pd.DataFrame(data)\n",
    "        user_testset_df[\"rating\"] = 0.0\n",
    "        user_testset_df[\"user\"] = user\n",
    "\n",
    "        testset = (\n",
    "            Dataset.load_from_df(\n",
    "                user_testset_df[[\"user\", \"item\", \"rating\"]],\n",
    "                reader=reader,\n",
    "            )\n",
    "            .build_full_trainset()\n",
    "            .build_testset()\n",
    "        )\n",
    "        \n",
    "        pred_list = model.test(testset)\n",
    "        \n",
    "        predictions = sorted(\n",
    "            [(pred.iid, pred.est)for pred in pred_list if ((pred.uid == user))],\n",
    "            key=lambda x: x[1],reverse=True\n",
    "        )\n",
    "        \n",
    "        reranked_list = rerank_recommendation(\n",
    "            user_profile_distribution,\n",
    "            predictions[:100],\n",
    "            user,\n",
    "            10,\n",
    "            lambda_\n",
    "        )\n",
    "        \n",
    "        \n",
    "        prediction_user_map[user] = reranked_list[1]\n",
    "\n",
    "                \n",
    "    return prediction_user_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1704b18",
   "metadata": {},
   "source": [
    "***Exercício 03:*** Explore como outros modelos se comportam com as injustiças relacionadas ao genero do filme. Visualize as métricas para diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa86aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b983ea",
   "metadata": {},
   "source": [
    "***Exercício 04:*** Qual o efeito provocado pela variação do parâmetro $\\lambda$ da calibração? Tente visualizar o efeito provocado nas métricas de fairness e acurácia se variarmos o parâmetro lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sistemas_manzato)",
   "language": "python",
   "name": "sistemas_manzato"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
