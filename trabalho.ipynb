{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6798e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c79c9f",
   "metadata": {},
   "source": [
    "# Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('./dataset/movies_sample.csv')\n",
    "ratings = pd.read_csv('./dataset/ratings_sample.csv')\n",
    "df = ratings[['userId', 'movieId', 'rating']]\n",
    "df = df.merge(movies[['movieId', 'title']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = df['userId'].unique()\n",
    "item_ids = df['movieId'].unique()\n",
    "\n",
    "map_users = {user: idx for idx, user in enumerate(user_ids)}\n",
    "map_items = {item: idx for idx, item in enumerate(item_ids)}\n",
    "\n",
    "df['userId'] = df['userId'].map(map_users)\n",
    "df['movieId'] = df['movieId'].map(map_items)\n",
    "\n",
    "\n",
    "original_id_to_title = movies.set_index('movieId')['title'].to_dict()\n",
    "map_title = {new_idx: original_id_to_title.get(old_id) for old_id, new_idx in map_items.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=.2, random_state=2)\n",
    "\n",
    "# Stats\n",
    "n_users = len(map_users)\n",
    "n_items = len(map_items)\n",
    "\n",
    "\n",
    "print(f\"Dados carregados: {len(df)} linhas.\")\n",
    "print(f\"Usuários únicos: {n_users}\")\n",
    "print(f\"Filmes únicos: {n_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cb0dc",
   "metadata": {},
   "source": [
    "# Implementação dos algoritmos de recomendação\n",
    "## Mais Populares\n",
    "Recomenda os itens mais popupares ainda não vistos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c89b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_counts = train.groupby('movieId').size().reset_index(name='count')\n",
    "pop_counts = pop_counts.sort_values('count', ascending=False)\n",
    "\n",
    "top_candidates = pop_counts['movieId'].values[:100]\n",
    "top_candidates_scores = pop_counts.set_index('movieId')['count'].to_dict()\n",
    "\n",
    "# Indexar histórico do usuário para verificação rápida \n",
    "user_seen_train = train.groupby('userId')['movieId'].apply(set).to_dict()\n",
    "\n",
    "# Gerar recomendações para usuários únicos do teste\n",
    "test_users = test['userId'].unique()\n",
    "N = 10\n",
    "recs_list = []\n",
    "\n",
    "print(f\"Gerando recomendações para {len(test_users)} usuários do teste...\")\n",
    "\n",
    "for user in test_users:\n",
    "    seen = user_seen_train.get(user, set())\n",
    "    user_recs = []\n",
    "    \n",
    "    for item in top_candidates:\n",
    "        if item not in seen:\n",
    "            user_recs.append(item)\n",
    "            if len(user_recs) == N:\n",
    "                break\n",
    "    \n",
    "    # Armazenar no formato para análise\n",
    "    for item in user_recs:\n",
    "        recs_list.append([user, item, top_candidates_scores[item]])\n",
    "\n",
    "# Criar DataFrame final de recomendações\n",
    "df_recs_pop = pd.DataFrame(recs_list, columns=['userId', 'movieId', 'pop_score'])\n",
    "\n",
    "# Adicionar Títulos para visualização\n",
    "df_recs_pop['title'] = df_recs_pop['movieId'].map(map_title)\n",
    "\n",
    "print(\"Recomendações geradas com sucesso.\")\n",
    "df_recs_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5726890",
   "metadata": {},
   "source": [
    "## UserKNN\n",
    "Recomenda baseado na preferência de usuários similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d482c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Criação da matriz esparsa\n",
    "R_sparse = csr_matrix(\n",
    "    (train['rating'].values, (train['userId'].values, train['movieId'].values)),\n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "user_sums = np.array(R_sparse.sum(axis=1)).flatten()\n",
    "user_counts = np.diff(R_sparse.indptr)\n",
    "# Evita divisão por zero\n",
    "user_means = np.zeros(n_users)\n",
    "mask = user_counts > 0\n",
    "user_means[mask] = user_sums[mask] / user_counts[mask]\n",
    "\n",
    "# Criar Matriz Centralizada (R - Média)\n",
    "R_centered = R_sparse.copy()\n",
    "# Subtrai a média do usuário de cada uma de suas avaliações não nulas\n",
    "R_centered.data -= np.repeat(user_means, user_counts)\n",
    "\n",
    "# Treinar Modelo na Matriz Centralizada\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=50, n_jobs=-1)\n",
    "model_knn.fit(R_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03957dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar recomendações\n",
    "def get_user_knn_recs(model, sparse_matrix, target_users, n_recs=10):\n",
    "    recs_list = []\n",
    "    \n",
    "    # Limitar para os MAX vizinhos\n",
    "    users_to_predict = target_users[:MAX] \n",
    "\n",
    "    # Encontrar vizinhos para o lote de usuários\n",
    "    # distances = 1 - similaridade (cosseno)\n",
    "    distances, indices = model.kneighbors(sparse_matrix[users_to_predict])\n",
    "    \n",
    "    for i, user_idx in enumerate(users_to_predict):\n",
    "        neighbor_indices = indices[i]\n",
    "        neighbor_distances = distances[i]\n",
    "        \n",
    "        # Similaridade = 1 - distância\n",
    "        similarities = 1 - neighbor_distances\n",
    "        \n",
    "        # Obter notas dos vizinhos\n",
    "        # neighbor_ratings shape: (k, n_items)\n",
    "        neighbor_ratings = sparse_matrix[neighbor_indices].toarray()\n",
    "        \n",
    "        #Predição de nota\n",
    "        # Score = Soma(Nota * Similaridade) / Soma(Similaridades)\n",
    "        # Ajuste dimensional para multiplicar (k, 1) * (k, n_items)\n",
    "        sim_col = similarities.reshape(-1, 1)\n",
    "        \n",
    "        weighted_sum = np.sum(neighbor_ratings * sim_col, axis=0)\n",
    "        \n",
    "        rated_mask = (neighbor_ratings > 0).astype(float)\n",
    "        sum_of_weights = np.dot(similarities, rated_mask) + 1e-9 # Evitar div/0\n",
    "        \n",
    "        predicted_scores = weighted_sum / sum_of_weights\n",
    "        \n",
    "        # Filtrar itens já vistos\n",
    "        user_row = sparse_matrix[user_idx].indices\n",
    "        predicted_scores[user_row] = -np.inf\n",
    "        \n",
    "        # Pegar Top N\n",
    "        top_indices = np.argpartition(predicted_scores, -n_recs)[-n_recs:]\n",
    "        top_indices = top_indices[np.argsort(predicted_scores[top_indices])[::-1]]\n",
    "        \n",
    "        for item_idx in top_indices:\n",
    "            score = predicted_scores[item_idx]\n",
    "            if score > 0 and score != -np.inf: # Garantir que é recomendação válida\n",
    "                recs_list.append([user_idx, item_idx, score])\n",
    "\n",
    "    return pd.DataFrame(recs_list, columns=['userId', 'movieId', 'knn_score'])\n",
    "\n",
    "# Executando\n",
    "unique_test_users = test['userId'].unique()\n",
    "df_recs_knn = get_user_knn_recs(model_knn, R_sparse, unique_test_users, n_recs=10)\n",
    "\n",
    "# Adicionando títulos\n",
    "df_recs_knn['title'] = df_recs_knn['movieId'].map(map_title)\n",
    "\n",
    "print(\"UserKNN finalizado.\")\n",
    "df_recs_knn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a9d42",
   "metadata": {},
   "source": [
    "## BPR\n",
    "Recomenda com base de interações implicitas, nesse caso consideraremos avaliações a cima de 3 como positivo e abaixo disso ou nao avaliado como negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b76bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "# Filtrar apenas interações positivas (>= 3) do treino\n",
    "train_pos = train[train['rating'] >= 3].copy()\n",
    "\n",
    "# Criar matriz esparsa UserxItem\n",
    "sparse_user_item = sparse.csr_matrix(\n",
    "    (np.ones(len(train_pos)), (train_pos['userId'], train_pos['movieId'])), \n",
    "    shape=(n_users, n_items)\n",
    ")\n",
    "\n",
    "print(f\"Matriz esparsa criada: {sparse_user_item.shape} com {sparse_user_item.nnz} interações positivas.\")\n",
    "\n",
    "# Configurar e Treinar o Modelo\n",
    "model_implicit = implicit.bpr.BayesianPersonalizedRanking(\n",
    "    factors=64, \n",
    "    learning_rate=0.01, \n",
    "    regularization=0.01, \n",
    "    iterations=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Treinando\n",
    "model_implicit.fit(sparse_user_item)\n",
    "\n",
    "# Gerar Recomendações\n",
    "\n",
    "recs_list_bpr = []\n",
    "unique_test_users = test['userId'].unique()\n",
    "\n",
    "users_to_rec = unique_test_users \n",
    "\n",
    "ids, scores = model_implicit.recommend(\n",
    "    users_to_rec, \n",
    "    sparse_user_item[users_to_rec], \n",
    "    N=10, \n",
    "    filter_already_liked_items=True\n",
    ")\n",
    "\n",
    "# Formatar a saída para o seu DataFrame padrão\n",
    "for i, user_id in enumerate(users_to_rec):\n",
    "    for j in range(len(ids[i])):\n",
    "        item_id = ids[i][j]\n",
    "        score = scores[i][j]\n",
    "        recs_list_bpr.append([user_id, item_id, score])\n",
    "\n",
    "df_recs_bpr = pd.DataFrame(recs_list_bpr, columns=['userId', 'movieId', 'bpr_score'])\n",
    "df_recs_bpr['title'] = df_recs_bpr['movieId'].map(map_title)\n",
    "\n",
    "df_recs_bpr.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca3ae7a",
   "metadata": {},
   "source": [
    "# Análise dos dados obtidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c6336",
   "metadata": {},
   "source": [
    "## Preparação dos metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de72f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('./dataset/movies_sample.csv')\n",
    "movies_df['movieId'] = movies_df['movieId'].map(map_items) # Usar o mesmo ID interno\n",
    "movies_df = movies_df.dropna(subset=['movieId']) # Remover itens que não entraram no map\n",
    "\n",
    "# Separar gêneros \n",
    "# dict: item_id -> lista de generos\n",
    "item_genres = {}\n",
    "for idx, row in movies_df.iterrows():\n",
    "    if row['genres'] != '(no genres listed)':\n",
    "        item_genres[int(row['movieId'])] = row['genres'].split('|')\n",
    "    else:\n",
    "        item_genres[int(row['movieId'])] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94789440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catalog_coverage(recs_df, n_items):\n",
    "    \"\"\"Calcula a porcentagem de itens únicos recomendados.\"\"\"\n",
    "    unique_recs = recs_df['movieId'].nunique()\n",
    "    coverage = (unique_recs / n_items) * 100\n",
    "    return coverage\n",
    "\n",
    "def get_gini_coefficient(recs_df, n_items):\n",
    "    \"\"\"\n",
    "    Calcula o Coeficiente de Gini da distribuição de recomendações.\n",
    "    Gini = 1: Apenas 1 item é recomendado para todos (Desigualdade Total).\n",
    "    Gini = 0: Todos os itens são recomendados igualmente (Igualdade Total).\n",
    "    \"\"\"\n",
    "    # Contagem de quantas vezes cada item foi recomendado\n",
    "    item_counts = recs_df['movieId'].value_counts().reindex(np.arange(n_items), fill_value=0).values\n",
    "    item_counts = np.sort(item_counts)\n",
    "    \n",
    "    # Cálculo do Gini\n",
    "    n = len(item_counts)\n",
    "    index = np.arange(1, n + 1)\n",
    "    gini = ((2 * np.sum(index * item_counts)) / (n * np.sum(item_counts))) - ((n + 1) / n)\n",
    "    return gini\n",
    "\n",
    "def calculate_shannon_entropy(genre_list):\n",
    "    \"\"\"Calcula a diversidade (entropia) de uma lista de gêneros.\"\"\"\n",
    "    if not genre_list:\n",
    "        return 0.0\n",
    "    counts = pd.Series(genre_list).value_counts(normalize=True) # Probabilidades\n",
    "    entropy = -np.sum(counts * np.log2(counts))\n",
    "    return entropy\n",
    "\n",
    "def analyze_filter_bubble(recs_df, train_df, target_users, item_genres_dict):\n",
    "    \"\"\"\n",
    "    Compara a entropia do histórico vs. recomendações.\n",
    "    Retorna a média de entropia do histórico e a média da recomendação.\n",
    "    \"\"\"\n",
    "    user_hist_entropy = []\n",
    "    user_recs_entropy = []\n",
    "    \n",
    "    # Agrupar histórico por usuário para acesso rápido\n",
    "    train_grouped = train_df[train_df['userId'].isin(target_users)].groupby('userId')['movieId'].apply(list)\n",
    "    recs_grouped = recs_df.groupby('userId')['movieId'].apply(list)\n",
    "    \n",
    "    for user in target_users:\n",
    "        # Entropia do Histórico\n",
    "        if user in train_grouped:\n",
    "            hist_items = train_grouped[user]\n",
    "            hist_genres = [g for item in hist_items for g in item_genres_dict.get(item, [])]\n",
    "            user_hist_entropy.append(calculate_shannon_entropy(hist_genres))\n",
    "        \n",
    "        # Entropia das Recomendações\n",
    "        if user in recs_grouped:\n",
    "            recs_items = recs_grouped[user]\n",
    "            recs_genres = [g for item in recs_items for g in item_genres_dict.get(item, [])]\n",
    "            user_recs_entropy.append(calculate_shannon_entropy(recs_genres))\n",
    "            \n",
    "    return np.mean(user_hist_entropy), np.mean(user_recs_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6580f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista dos dataframes e nomes\n",
    "models = [\n",
    "    ('Most Popular', df_recs_pop),\n",
    "    ('UserKNN', df_recs_knn),\n",
    "    ('BPR-MF', df_recs_bpr)\n",
    "]\n",
    "\n",
    "# Variáveis para armazenar resultados\n",
    "results = []\n",
    "target_users_analysis = df_recs_bpr['userId'].unique() # Usar mesmos usuários para todos\n",
    "\n",
    "print(f\"{'Modelo':<15} | {'Cob. (%)':<10} | {'Gini (Viés)':<12} | {'Ent. Hist':<10} | {'Ent. Recs':<10} | {'Delta Entropia':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, df in models:\n",
    "    # Cobertura\n",
    "    cov = get_catalog_coverage(df, n_items)\n",
    "    \n",
    "    # Gini\n",
    "    gini = get_gini_coefficient(df, n_items)\n",
    "    \n",
    "    # Bolha de Filtro \n",
    "    df_filtered = df[df['userId'].isin(target_users_analysis)]\n",
    "    ent_hist, ent_recs = analyze_filter_bubble(df_filtered, train, target_users_analysis, item_genres)\n",
    "    \n",
    "    # Delta: Se negativo, reduziu a diversidade (Bolha). Se positivo, expandiu.\n",
    "    delta = ent_recs - ent_hist\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': name, 'Cobertura': cov, 'Gini': gini, \n",
    "        'Entropia_Hist': ent_hist, 'Entropia_Recs': ent_recs\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<15} | {cov:>9.2f}% | {gini:>11.4f}  | {ent_hist:>9.2f}  | {ent_recs:>9.2f}  | {delta:>14.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_long_tail_density_linear(models_data, train_df, n_items):\n",
    "    # 1. Definir Ranking de Popularidade Real (Treino)\n",
    "    item_popularity = train_df['movieId'].value_counts().reindex(np.arange(n_items), fill_value=0)\n",
    "    sorted_items = item_popularity.sort_values(ascending=False).index\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    colors = {'Most Popular': '#d62728', 'UserKNN': '#1f77b4', 'BPR-MF': '#2ca02c'}\n",
    "    \n",
    "    # Janelas de suavização\n",
    "    smoothing_windows = {'Most Popular': 20, 'UserKNN': 200, 'BPR-MF': 200}\n",
    "    \n",
    "    for name, df in models_data:\n",
    "        # Contagem bruta\n",
    "        rec_counts = df['movieId'].value_counts().reindex(sorted_items, fill_value=0)\n",
    "\n",
    "        # Normalização: densidade\n",
    "        rec_density = rec_counts / rec_counts.sum()\n",
    "\n",
    "        # Suavização\n",
    "        window = smoothing_windows.get(name, 100)\n",
    "        smooth_curve = rec_density.rolling(window=window, min_periods=1, center=True).mean()\n",
    "        \n",
    "        x_axis = np.arange(len(smooth_curve))\n",
    "        \n",
    "        # Plot\n",
    "        plt.plot(x_axis, smooth_curve, label=name, color=colors[name], linewidth=2.5)\n",
    "\n",
    "        # Sombreamento\n",
    "        plt.fill_between(x_axis, 0, smooth_curve, color=colors[name], alpha=0.1)\n",
    "\n",
    "        # Marcador do ponto onde o modelo chega a 99.9% da massa\n",
    "        threshold = 0.999\n",
    "        cumsum_curve = smooth_curve.cumsum()\n",
    "        last_idx = np.argmax(cumsum_curve >= threshold)\n",
    "\n",
    "\n",
    "    plt.title('Fenômeno da Cauda Longa nas recomendações', fontsize=14)\n",
    "    plt.xlabel('Itens', fontsize=12)\n",
    "    plt.ylabel('Popularidade', fontsize=12)\n",
    "\n",
    "    plt.xlim(0, n_items)\n",
    "    plt.ylim(bottom=0, top=max(smooth_curve.max() for _, df in models_data) * 1.1)\n",
    "\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "real_n_items = len(map_items)\n",
    "plot_long_tail_density_linear(models, train, real_n_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Most Popular\", \"UserKNN\", \"BPR-MF\"]\n",
    "values = [11.99, 77.46, 36.45]\n",
    "colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(labels, values, color=colors)\n",
    "\n",
    "plt.title(\"Catalog Coverage por Modelo\")\n",
    "plt.ylabel(\"Catalog Coverage (%)\")\n",
    "plt.xlabel(\"Modelos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
